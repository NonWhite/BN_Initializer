\section{Introduction}

\subsection{Probability Theory}
	\begin{frame}
		\begin{itemize}
			\item Variables $X_1, \ldots , X_n$ takes values in $\Omega_1 , \ldots , \Omega_n$
				\begin{itemize}
					\item $X_1: {Gender} , \Omega_1 = \{ {Male} , {Female} \}$
					\item $X_2: {City\ size} , \Omega_2 = \{ {Big} , {Small} \}$
				\end{itemize}
			\item Factored possibility space $\Omega = \Omega_1 \times \ldots \times \Omega_n$
			\item Event is a subset of $\Omega$
			\item Probability function maps events $\alpha$ and $\beta$ into real values such that
			\begin{itemize}
				\item $0 \leq \P( \alpha ) \leq 1 $
				\item $\P( \Omega ) = 1$
				\item $\P( \alpha \cup \beta ) = \P( \alpha ) + \P( \beta ) - \P( \alpha \cap \beta )$
			\end{itemize}
		\end{itemize}
	\end{frame}

\subsection{Probability Distribution of a Variable}
	\begin{frame}
		\begin{itemize}
			\item Every assignment of value to a variable correspond to an event:
				\[ {Gender} = M \leftrightarrow \alpha = \{ ( M , s ) , ( M , b ) \} \]
			\item The probability distribution of a variable $X$ maps assignments of the variable to the respective probabilities:
				\[ \P( X = x ) = \P( \{ \omega : \omega \text{ consistent with } x \} ) \]
			\item We denote the probability distribution of $X$ as $\P( X )$ and the probability of an arbitrary event $\{ X = x \}$ as $\P( x )$
			\item It follows from the properties of probability function that
				\[ \sum_X \P( X ) = \sum_{x \in \Omega_X} \P( X = x ) = 1 \]
		\end{itemize}
	\end{frame}

\subsection{Joint Probability Distribution}
	\begin{frame}
		\begin{itemize}
			\item A joint assignment to a set of variables is an event
				\[ {Gender} = M \text{ and } {City\ size} = b \leftrightarrow \alpha = \{ ( M , b ) \} \]
			\item The joint probability distribution of a set of variables is a function that maps joint assignments to their event probabilities:
				\[ \P( X = x , Y = y ) = \P( \{ \omega : \omega \text{ consistent with } x, y  \} ) \]
			\item We denote the probability distribution of $X$ and $Y$ as $\P( X )$ and the probability of an arbitrary joint event as $\P( x , y )$
			\item It follows from the properties of probability function that
				\[ \sum_{X,Y} \P( X , Y ) = \sum_{x \in \Omega_X} \sum_{x \in \Omega_Y}\P( X = x , Y = y ) = 1 \]
		\end{itemize}
	\end{frame}

\subsection{Conditional Probability Distribution}
	\begin{frame}
		\begin{itemize}
			\item Maps assignments of two variables to conditional probabilities:
				\[ \P( X = x \mid Y = y ) = \frac{\P( X = x , Y = y )}{\P( Y = y )} \]
			\item Represented as $\P( X \mid Y )$
			\item Analogously, we can define join conditional probability distribution $\P( X , Y \mid Z , W )$
		\end{itemize}
	\end{frame}

\subsection{Chain Rule}
	\begin{frame}
		By definition of conditional probability:
			\[ \P( \alpha \mid \beta ) \P( \beta ) = \P( \alpha \cap \beta ) \]
		For events $\alpha_1 , \ldots , \alpha_n$ it follows that
			\[ \P( \alpha_1 \cap \ldots \cap \alpha_n ) = \P( \alpha_1 ) \prod_{i=2}^{n} \P( \alpha_i \mid \alpha_1 \cap \ldots \cap \alpha_{i-1} ) \]
		In terms of variables:
			\[ \P( A , B , C ) = \P( A ) \P( B \mid A ) \P( C \mid B , A ) \]
	\end{frame}

\subsection{Bayes' Rule}
	\begin{frame}
		\[ \P( \beta \mid \alpha ) = \frac{\P( \alpha \mid \beta )}{\P( \alpha )} \P( \beta ) \]
		\begin{itemize}
			\item Prior probability: $\P( \beta )$
			\item Posterior probability: $\P( \beta \mid \alpha )$
			\item Data Likelihood: $\P( \alpha \mid \beta )$
			\item Evidence probability: $\P( \alpha )$
		\end{itemize}
		\begin{itemize}
			\item Bayes' rule can be seen as a way of \alert{revising beliefs} in light of new information/knowledge: start with $\P( \beta )$, observe $\alpha$ then set $\P( \beta )' = \P( \beta \mid \alpha )$
			\item This way of thinking is known as \alert{Bayesian Reasoning}
		\end{itemize}
	\end{frame}

\subsection{Probabilistic Independence}
	\begin{frame}
		Events $\alpha$ and $\beta$ are independent if:
			\[ \P( \alpha \cap \beta ) = \P( \alpha ) \P( \beta ) \]
		\begin{itemize}
			\item The following are equivalent definitions:
			\begin{itemize}
				\item Either $\P( \alpha \mid \beta ) = \P( \alpha ) \text{ or } \P( \beta ) = 0$
				\item Either $\P( \beta \mid \alpha ) = \P( \beta ) \text{ or } \P( \alpha ) = 0$
			\end{itemize}
			\item Knowing $\beta$ is irrelevant to determining the value of $\alpha$
			\item Knowing $\alpha$ is irrelevant to determining the value of $\beta$
		\end{itemize}
	\end{frame}
	\begin{frame}
		Variables $A$ and $B$ are independent if:
			\[ \P( A = a , B = b ) = \P( A = a ) \P( B = b ) \]
		for all values of $a$ and $b$.\\
		Another way to write this is:
			\[ \P( A , B ) = \P( A ) \P( B ) \]
	\end{frame}

\subsection{Conditional Independence}
	\begin{frame}
		Events $\alpha$ and $\beta$ are independent conditional on event $\gamma$ if:
			\[ \P( \alpha \cap \beta \mid \gamma ) = \P( \alpha \mid \gamma ) \P( \beta \mid \gamma ) \]
		The following are equivalent definitions:
		\begin{itemize}
			\item Either $\P( \alpha \mid \beta , \gamma ) = \P( \alpha \mid \gamma ) \text{ or } \P( \beta \mid \gamma ) = 0$
			\item Either $\P( \beta \mid \alpha , \gamma ) = \P( \beta \mid \gamma ) \text{ or } \P( \alpha \mid \gamma ) = 0$
		\end{itemize}
		Analogously, variables $A$ and $B$ are conditionally independent given $C$ if
			\[ \P( A , B \mid C ) = \P( A \mid C ) \P( B \mid C ) \]
		for every assignment to $A$, $B$ and $C$
	\end{frame}
	
\subsection{Model Complexity}
	\begin{frame}
		The presence of independences reduces the number of probability values to specify:
		\begin{itemize}
			\item \alert{No independences}: $\P( A , B , C )$, $k^3$ values
			\item $A$, $B$ and $C$ are \alert{dependent}, and $A$ and $B$ are \alert{conditionally independent} given $C$: $\P( A , B \mid C ) = \P( A \mid C ) \P( B \mid C )$ , $k + 2k^2$ values
			\item $A$, $B$ and $C$ are \alert{independent}: $\P( A , B , C ) = \P( A ) \P( B ) \P( C )$ , $3k$ values
		\end{itemize}
	\end{frame}