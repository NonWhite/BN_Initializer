\section{Experiments, Results and Discussion}
\label{sec:experiments}

In order to evaluate the quality of our approaches, we learned Bayesian networks using Order-based greedy search and different initialization strategies from several data sets commonly used for benchmarking. The names and relevant characteristics of the data sets\footnote{These datasets were extracted from http://urlearning.org/datasets.html} used are shown in Table~\ref{tab:datasets}, where the density of a graph is defined as the division of its number of edges and its number of nodes.
	\begin{table}[ h ]
		\centering
		\begin{tabular}{ | l | c | c | c | }
			\hline
			Dataset & n (\#attributes) & N (\#instances) & Density of $\overline G$ \\ \hline
			Census & 15 & 30168 & 2.85 \\ \hline
			Letter & 17 & 20000 & 2.41 \\ \hline
			Image & 20 & 2310 & 2.45 \\ \hline
			Mushroom & 23 & 8124 & 2.91 \\ \hline
			Sensors & 25 & 5456 & 3.00 \\ \hline
			SteelPlates & 28 & 1941 & 2.18 \\ \hline
			Epigenetics & 30 & 72228 & 1.87 \\ \hline
			Alarm & 37 & 1000 & 1.98 \\ \hline
			Spectf & 45 & 267 & 1.76 \\ \hline
			LungCancer & 57 & 27 & 1.44 \\ \hline
		\end{tabular}
		\caption{Data sets characteristics}
		\label{tab:datasets}
	\end{table}
The greedy search was ran with a maximum number of parents ($d=3$), a limit of 100 iterations ($K=100$), and 1000 restarts ($S=1000$), except for LungCancer dataset where the running total time is big to perform such as number of restarts. We used the BIC score in all experiments and the parent sets were calculated by exhaustive search in all cases.

We compared our proposed initialization strategies, DFS- and FAS-based, against the standard approach of randomly generation a order. For each approach, we compared the best score obtained, the average initial score, the average best score and the average number of iterations that local search took to converge. The results are shown in Table~\ref{tab:comparison}.
	\input{addons/results}
The results show that in most of the datasets with less than 25 attributes, the random approach finds Bayesian networks with greater or equal score than the methods proposed, but it does not hold for bigger datasets where DFS-based method obtained better scores than FAS-based because the reduction in the search space done by these methods improves the possibilities to obtain better scores as shown in average best score column, also with a considerable lower standard deviation compared to the random approach. Moreover, results show that new methods work better in datasets where graph $\overline G$ is sparse (small density), possible because it has to do less choices for removing arcs in $\overline G$. Finally, the relation between the average number of iterations needed to converge from DFS-based method and random approach is increasing while $n$ increases. However, FAS-method initial solutions converge faster than in any of the methods with a considerable difference which means that these initial solutions are close to good local maxima, even for the biggest datasets. Last statement can be proven to be true by looking at the average initial score column that shows that FAS-based method always generate solutions with the highest scores having the lowest standard deviation. So in general, the new methods work better with bigger datasets than smaller ones and this improves while the density of $\overline G$ decreases.

% Converge curves
% \input{addons/curves}
