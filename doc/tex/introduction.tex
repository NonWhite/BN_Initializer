\section{Introduction}
\label{sec:introduction}

Bayesian Networks are space-efficient representations of multivariate probability distributions over random variables~\cite{VJensen01}. They are defined by two components: (i) a directed acyclic graph (DAG), where the nodes are the variables and the edges encode the (in)dependence relationships among the variables; and (ii) a collection of local conditional probability distributions of each variable given its parents. More formally, a Bayesian network specification contains a DAG \( G = ( V , E ), \) where $V = \{ X_1 , X_2 , \ldots , X_n \}$ is the set of (discrete) variables, and a collection of conditional probability distributions \( P( X_i \mid {Pa}_G( X_i ) ) \), $i=1,\ldots,n$, where ${Pa}_G( X_i )$ is the set of variables that are parents of $X_i$ in $G$. This definition shows that the number of numerical parameters (i.e., local conditional probability values) grows exponentially with the number of parents (in-degree) of a node (assuming the values are organized in tables). A Bayesian network induces a joint probability distribution over all the variables through the equation
	\begin{equation}
		\label{eq:jointdist}
		P( X_1 , X_2 , \ldots , X_n ) = \prod_{i=1}^{n} P( X_i \mid {Pa}_G( X_i ) ).
	\end{equation}
Hence, Bayesian networks with sparse DAGs succinctly represent joint probability distributions over many variables.

A common approach to learning Bayesian networks from a given data set is score-based. These methods such as K2 algorithm~\cite{Cooper92}, consists of associating every graph structure with a polynomial-time computable score value and searching for structures with high score values~\cite{Margaritis03}. However, there is another approach to solve this problem that is a constraint-based one where the methods learn conditional independence statements among the variables using statistical tests on the data set. With these statements the methods build an undirected structure and then determine the edge's orientation to obtain a Bayesian network~\cite{Spirtes95}, but these are not the focus in this work, so we will not to these any further

The score value of a structure usually rewards structures that assign high probability of observing the data set (i.e., the data likelihood) and penalizes the complexity of the model (i.e., the number of parameters). Some examples are the Bayesian Information Criterion (BIC)~\cite{BIC91}, the Minimum Description Length (MDL)~\cite{MDL94} and the Bayesian Dirichlet score (BD)~\cite{BD95}.

Score-based Bayesian network structure learning from data is a NP-hard problem~\cite{MSResearch04}, even when the in-degree (i.e., maximum number of parents) of the graph is bounded. For this reason, the most common approach to solve the problem is to use local search methods such as searching over the space of DAGs~\cite{FNP99}, searching over Markov equivalence classes~\cite{Maxwell02}, and searching over the space of topological orders~\cite{TK05}. Local search methods are well-known to be vulnerable to poor local maxima unless a strategy for avoiding low score regions is used. One such strategy is the use of non-greedy heuristics that allow moving for lower value solutions during search in order to escape local maxima~\cite{ENF02}. Another strategy is to use \emph{good initialization heuristics} that attempt to start the search in regions of high local maxima. Most often, a simple uniformed random generation of initial solutions is adopted.

In this article we propose new heuristics for generating good initial solutions to be fed into local search methods for Bayesian network structure learning. We focus on order-based greedy methods on data sets which are state of the art. Also, one of the heuristics is motivated by solutions of the Feedback Arc Set Problem (FASP), which is the problem of transforming a cyclic direct graph into a DAG. Our experiments show that using these new methods improves the quality of order-based local search.

The article is structured as follows: we begin in Section~\ref{sec:learning} explaining the Greedy Search approach to learning Bayesian networks. In Section~\ref{sec:improve}, we describe the new algorithms for generating initial solutions. Section~\ref{sec:experiments} shows the experiments using both approaches and comparing them (in scoring and number of iterations needed) with multiple data sets. Finally, in Section~\ref{sec:conclusions} we give
some conclusions about the new methods.
